{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "3. Lemmatization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimanshuSahoo/NLP/blob/main/3_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94fa0e40"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "id": "94fa0e40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d116f4f"
      },
      "source": [
        "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
        "               Thank you to all of you in this room. I have to congratulate \n",
        "               the other incredible nominees this year. The Revenant was \n",
        "               the product of the tireless efforts of an unbelievable cast\n",
        "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
        "               Hardy. Tom, your talent on screen can only be surpassed by \n",
        "               your friendship off screen … thank you for creating a t\n",
        "               ranscendent cinematic experience. Thank you to everybody at \n",
        "               Fox and New Regency … my entire team. I have to thank \n",
        "               everyone from the very onset of my career … To my parents; \n",
        "               none of this would be possible without you. And to my \n",
        "               friends, I love you dearly; you know who you are. And lastly,\n",
        "               I just want to say this: Making The Revenant was about\n",
        "               man's relationship to the natural world. A world that we\n",
        "               collectively felt in 2015 as the hottest year in recorded\n",
        "               history. Our production needed to move to the southern\n",
        "               tip of this planet just to be able to find snow. Climate\n",
        "               change is real, it is happening right now. It is the most\n",
        "               urgent threat facing our entire species, and we need to work\n",
        "               collectively together and stop procrastinating. We need to\n",
        "               support leaders around the world who do not speak for the \n",
        "               big polluters, but who speak for all of humanity, for the\n",
        "               indigenous people of the world, for the billions and \n",
        "               billions of underprivileged people out there who would be\n",
        "               most affected by this. For our children’s children, and \n",
        "               for those people out there whose voices have been drowned\n",
        "               out by the politics of greed. I thank you all for this \n",
        "               amazing award tonight. Let us not take this planet for \n",
        "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
      ],
      "id": "7d116f4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ac3420"
      },
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "id": "28ac3420",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4f7ddd"
      },
      "source": [
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)  "
      ],
      "id": "4a4f7ddd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f9b61d2",
        "outputId": "c54e6c94-014d-451e-b914-f47e1e65c94e"
      },
      "source": [
        "sentences"
      ],
      "id": "2f9b61d2",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Thank much .',\n",
              " 'Thank Academy .',\n",
              " 'Thank room .',\n",
              " 'I congratulate incredible nominee year .',\n",
              " 'The Revenant product tireless effort unbelievable cast crew .',\n",
              " 'First , brother endeavor , Mr. Tom Hardy .',\n",
              " 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .',\n",
              " 'Thank everybody Fox New Regency … entire team .',\n",
              " 'I thank everyone onset career … To parent ; none would possible without .',\n",
              " 'And friend , I love dearly ; know .',\n",
              " \"And lastly , I want say : Making The Revenant man 's relationship natural world .\",\n",
              " 'A world collectively felt 2015 hottest year recorded history .',\n",
              " 'Our production needed move southern tip planet able find snow .',\n",
              " 'Climate change real , happening right .',\n",
              " 'It urgent threat facing entire specie , need work collectively together stop procrastinating .',\n",
              " 'We need support leader around world speak big polluter , speak humanity , indigenous people world , billion billion underprivileged people would affected .',\n",
              " 'For child ’ child , people whose voice drowned politics greed .',\n",
              " 'I thank amazing award tonight .',\n",
              " 'Let u take planet granted .',\n",
              " 'I take tonight granted .',\n",
              " 'Thank much .']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "372478f9"
      },
      "source": [
        ""
      ],
      "id": "372478f9",
      "execution_count": null,
      "outputs": []
    }
  ]
}